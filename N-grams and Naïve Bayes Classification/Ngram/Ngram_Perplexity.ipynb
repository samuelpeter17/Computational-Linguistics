{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXcge0wSzBb6"
      },
      "source": [
        "# N-gram probabilities and perplexity\n",
        "Samuel Peter (samuel.peter.25@dartmouth.edu)<br>\n",
        "Dartmouth College, LING48, Spring 2024\n",
        "\n",
        "Documentation of the NLTK.LM package:<br>\n",
        "https://www.nltk.org/api/nltk.lm.html\n",
        "\n",
        "Tip 1: How to extract n-gram probabilities:<br>\n",
        "https://stackoverflow.com/questions/54962539/how-to-get-the-probability-of-bigrams-in-a-text-of-sentences\n",
        "\n",
        "Tip 2: Calculating perplexity with NLTK:<br>\n",
        "https://stackoverflow.com/questions/54941966/how-can-i-calculate-perplexity-using-nltk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXbg5lhzpAaA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ae6a63a-1c52-49ee-ef3f-a4cf00ae22c0"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1vCMVOyo8AA"
      },
      "source": [
        "import os\n",
        "import requests\n",
        "import io\n",
        "import random\n",
        "from nltk.lm.preprocessing import pad_both_ends, padded_everygram_pipeline\n",
        "from nltk.lm import MLE, NgramCounter, Vocabulary\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "from nltk import word_tokenize, sent_tokenize, bigrams, trigrams\n",
        "import gdown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLyEbEgUq-RW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "115ab970-5129-41a5-bc87-e7c9471bbef9"
      },
      "source": [
        "# Download and decompress corpora\n",
        "url = \"https://drive.google.com/uc?id=1DAkd5C7HRTy0Tv2nSIWdpa4PMcKe5yZi\"\n",
        "output = 'hw4-corpora-2024.zip'\n",
        "gdown.download(url, output, quiet=False)\n",
        "!unzip -j hw4-corpora-2024.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1DAkd5C7HRTy0Tv2nSIWdpa4PMcKe5yZi\n",
            "From (redirected): https://drive.google.com/uc?id=1DAkd5C7HRTy0Tv2nSIWdpa4PMcKe5yZi&confirm=t&uuid=c4666b05-8b7e-4cb3-a220-8a945787fc3e\n",
            "To: /content/hw4-corpora-2024.zip\n",
            "100%|██████████| 31.2M/31.2M [00:00<00:00, 140MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  hw4-corpora-2024.zip\n",
            "  inflating: amharic-converted.txt   \n",
            "  inflating: arabic-nawal-sadawi.txt  \n",
            "  inflating: bangla-wiki.txt         \n",
            "  inflating: english-shakespeare.txt  \n",
            "  inflating: english-sherlock.txt    \n",
            "  inflating: french-victor-hugo.txt  \n",
            "  inflating: german-kafka.txt        \n",
            "  inflating: greek-europarl-greek.txt  \n",
            "  inflating: gujarati-ai4bharat.txt  \n",
            "  inflating: hindi-jansatta-utf8.txt  \n",
            "  inflating: igbo-corpus.txt         \n",
            "  inflating: indonesian-wikipedia-sentences.txt  \n",
            "  inflating: japanese-natsume-soseki.txt  \n",
            "  inflating: kinyarwanda-corpus.txt  \n",
            "  inflating: korean-news.txt         \n",
            "  inflating: latin-virgil.txt        \n",
            "  inflating: mandarin-lu-xun.txt     \n",
            "  inflating: marathi-ai4bharat.txt   \n",
            "  inflating: mauritian-creole-corpus.txt  \n",
            "  inflating: navajo-wikipedia-10k.txt  \n",
            "  inflating: nepali-artha-banijya.txt  \n",
            "  inflating: norwegian-bokmal-sigrid-undset.txt  \n",
            "  inflating: odia-ai4bharat.txt      \n",
            "  inflating: polish-europarl-polish.txt  \n",
            "  inflating: portuguese-clarice-lispector.txt  \n",
            "  inflating: punjabi-6500lines-ai4bharat(1).txt  \n",
            "  inflating: russian-pushkin-poems.txt  \n",
            "  inflating: spanish-garcia-marquez.txt  \n",
            "  inflating: swahili-sw-daima.txt    \n",
            "  inflating: tagalog.txt             \n",
            "  inflating: tamil-wiki.txt          \n",
            "  inflating: telugu-ai4bharat.txt    \n",
            "  inflating: thai-khun-chang-khun-phaen.txt  \n",
            "  inflating: tigrinya-converted.txt  \n",
            "  inflating: turkish-news-leipzig.txt  \n",
            "  inflating: urdu-news.txt           \n",
            "  inflating: vietnamese-news.txt     \n",
            "  inflating: yoruba-wikipedia-leipzig.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWECRfnWp02P"
      },
      "source": [
        "# Open file\n",
        "file = io.open('german-kafka.txt', encoding='utf8')\n",
        "text = file.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YA-utVqO2goN"
      },
      "source": [
        "# Preprocess the tokenized text for language modelling\n",
        "https://stackoverflow.com/questions/54959340/nltk-language-modeling-confusion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqfeSN9X2eOl"
      },
      "source": [
        "# Preprocess the tokenized text for language modelling\n",
        "n = 2\n",
        "paddedLine = [list(pad_both_ends(word_tokenize(text.lower()), n))]\n",
        "train, vocab = padded_everygram_pipeline(n, paddedLine)\n",
        "\n",
        "# Train a n-gram maximum likelihood estimation model.\n",
        "model = MLE(n)\n",
        "model.fit(train, vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUZIWG1q5aa0"
      },
      "source": [
        "#How to extract n-gram probabilities:<br>\n",
        "https://stackoverflow.com/questions/54962539/how-to-get-the-probability-of-bigrams-in-a-text-of-sentences\n",
        "\n",
        "#Calculating perplexity with NLTK:<br>\n",
        "https://stackoverflow.com/questions/54941966/how-can-i-calculate-perplexity-using-nltk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJNrb2fI5W5U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0699059-31a5-43f7-9290-a5040297343b"
      },
      "source": [
        "# NLTK will calculate the perplexity of these sentences\n",
        "test_sentences = ['Ich habe zwar von irgend', 'deinetwegen will ich nicht widerstehen', 'ich will Fußball spielen']\n",
        "tokenized_text = [list(map(str.lower, word_tokenize(sent))) for sent in test_sentences]\n",
        "\n",
        "# Probability of bigrams\n",
        "test_data = [bigrams(t,  pad_right=False, pad_left=False) for t in tokenized_text]\n",
        "for test in test_data:\n",
        "    print (\"MLE Estimates:\", [((ngram[-1], ngram[:-1]),model.score(ngram[-1], ngram[:-1])) for ngram in test])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLE Estimates: [(('habe', ('ich',)), 0.050565626277770205), (('zwar', ('habe',)), 0.005199306759098787), (('von', ('zwar',)), 0.010416666666666666), (('irgend', ('von',)), 0.0007309941520467836)]\n",
            "MLE Estimates: [(('will', ('deinetwegen',)), 0.125), (('ich', ('will',)), 0.1860986547085202), (('nicht', ('ich',)), 0.03380128117759302), (('widerstehen', ('nicht',)), 0.0006740361283364788)]\n",
            "MLE Estimates: [(('will', ('ich',)), 0.016764345100177182), (('fußball', ('will',)), 0.0), (('spielen', ('fußball',)), 0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZEXKD_j5vkL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99b0a7e8-248c-4735-8e0e-55786f9ab308"
      },
      "source": [
        "# Perplexity of bigrams\n",
        "test_data = [bigrams(t,  pad_right=False, pad_left=False) for t in tokenized_text]\n",
        "for i, test in enumerate(test_data):\n",
        "  print(\"PP({0}):{1}\".format(test_sentences[i], model.perplexity(test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PP(Ich habe zwar von irgend):149.4992880929327\n",
            "PP(deinetwegen will ich nicht widerstehen):37.062320219847486\n",
            "PP(ich will Fußball spielen):inf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Result Analysis\n",
        "- **Lowest Perplexity Sentence (Generated by the Model):**\n",
        "  - Sentence: \"deinetwegen will ich nicht widerstehen\"\n",
        "  - Perplexity: 37.06\n",
        "  - This sentence has the lowest perplexity among the sentences generated by the model. The lower perplexity suggests that the trigram model itself finds it easier to predict the next word in this sentence compared to other sentences it generated. This indicates that the model is more confident and accurate in generating this particular sentence.\n",
        "\n",
        "- **Highest Perplexity Sentence:**\n",
        "  - Sentence: \"ich will Fußball spielen\"\n",
        "  - Perplexity: Infinity (inf)\n",
        "  - This sentence has the highest perplexity among the sentences generated by the model. The perplexity value of infinity indicates that the trigram model encountered a sequence that it could not predict effectively. This suggests that the model struggled with this specific sentence, likely due to the presence of less common or unseen words or word sequences. In this case, the unseen word for the model is: Fußball.\n",
        "\n",
        "In summary, when considering the generated sentences, the lowest perplexity still indicates better predictability and confidence in the model's ability to generate coherent text, while the highest perplexity reflects difficulties in generating sentences containing less common or unseen word sequences."
      ],
      "metadata": {
        "id": "U2CIjf_srYId"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qsUqfcqoo7bp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}