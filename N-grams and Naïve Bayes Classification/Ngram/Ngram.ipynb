{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXcge0wSzBb6"
      },
      "source": [
        "#N-gram probabilities and n-gram text generation\n",
        "Samuel Peter (samuel.peter.25@dartmouth.edu)<br>\n",
        "Dartmouth College, LING48, Spring 2024\n",
        "\n",
        "Documentation of the NLTK.LM package:<br>\n",
        "https://www.nltk.org/api/nltk.lm.html\n",
        "\n",
        "Quick tip: How to extract n-gram probabilities:<br>\n",
        "https://stackoverflow.com/questions/54962539/how-to-get-the-probability-of-bigrams-in-a-text-of-sentences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install googletrans==4.0.0-rc1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccd3q3bzJOHK",
        "outputId": "7228467d-8186-43f1-f90b-78d1418c3e1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m770.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.2.2)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2024.4.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17395 sha256=2eeba2a949e78f05f0756b2e7ff7e39acda9b673f738c486d0816711d2a1d818\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.7\n",
            "    Uninstalling idna-3.7:\n",
            "      Successfully uninstalled idna-3.7\n",
            "Successfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.4.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "-vbb6E9JxLjO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7883c19-1b08-4258-d729-d2024d1e0935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1vCMVOyo8AA"
      },
      "source": [
        "import os\n",
        "import requests\n",
        "import io\n",
        "import random\n",
        "from nltk.lm.preprocessing import pad_both_ends, padded_everygram_pipeline\n",
        "from nltk.lm import MLE, NgramCounter, Vocabulary\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "from nltk import word_tokenize, sent_tokenize, bigrams, trigrams\n",
        "import gdown\n",
        "from googletrans import Translator\n",
        "from collections import Counter\n",
        "from google.colab import files\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLyEbEgUq-RW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64a15c28-dda3-475f-e64a-63eff18a8f46"
      },
      "source": [
        "# Download and decompress corpora from the homework\n",
        "url = \"https://drive.google.com/uc?id=1DAkd5C7HRTy0Tv2nSIWdpa4PMcKe5yZi\"\n",
        "output = 'hw4-corpora-2024.zip'\n",
        "gdown.download(url, output, quiet=False)\n",
        "!unzip -j hw4-corpora-2024.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1DAkd5C7HRTy0Tv2nSIWdpa4PMcKe5yZi\n",
            "From (redirected): https://drive.google.com/uc?id=1DAkd5C7HRTy0Tv2nSIWdpa4PMcKe5yZi&confirm=t&uuid=b1cbf4b9-f5e5-433b-87ae-8808e464a927\n",
            "To: /content/hw4-corpora-2024.zip\n",
            "100%|██████████| 31.2M/31.2M [00:01<00:00, 24.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  hw4-corpora-2024.zip\n",
            "  inflating: amharic-converted.txt   \n",
            "  inflating: arabic-nawal-sadawi.txt  \n",
            "  inflating: bangla-wiki.txt         \n",
            "  inflating: english-shakespeare.txt  \n",
            "  inflating: english-sherlock.txt    \n",
            "  inflating: french-victor-hugo.txt  \n",
            "  inflating: german-kafka.txt        \n",
            "  inflating: greek-europarl-greek.txt  \n",
            "  inflating: gujarati-ai4bharat.txt  \n",
            "  inflating: hindi-jansatta-utf8.txt  \n",
            "  inflating: igbo-corpus.txt         \n",
            "  inflating: indonesian-wikipedia-sentences.txt  \n",
            "  inflating: japanese-natsume-soseki.txt  \n",
            "  inflating: kinyarwanda-corpus.txt  \n",
            "  inflating: korean-news.txt         \n",
            "  inflating: latin-virgil.txt        \n",
            "  inflating: mandarin-lu-xun.txt     \n",
            "  inflating: marathi-ai4bharat.txt   \n",
            "  inflating: mauritian-creole-corpus.txt  \n",
            "  inflating: navajo-wikipedia-10k.txt  \n",
            "  inflating: nepali-artha-banijya.txt  \n",
            "  inflating: norwegian-bokmal-sigrid-undset.txt  \n",
            "  inflating: odia-ai4bharat.txt      \n",
            "  inflating: polish-europarl-polish.txt  \n",
            "  inflating: portuguese-clarice-lispector.txt  \n",
            "  inflating: punjabi-6500lines-ai4bharat(1).txt  \n",
            "  inflating: russian-pushkin-poems.txt  \n",
            "  inflating: spanish-garcia-marquez.txt  \n",
            "  inflating: swahili-sw-daima.txt    \n",
            "  inflating: tagalog.txt             \n",
            "  inflating: tamil-wiki.txt          \n",
            "  inflating: telugu-ai4bharat.txt    \n",
            "  inflating: thai-khun-chang-khun-phaen.txt  \n",
            "  inflating: tigrinya-converted.txt  \n",
            "  inflating: turkish-news-leipzig.txt  \n",
            "  inflating: urdu-news.txt           \n",
            "  inflating: vietnamese-news.txt     \n",
            "  inflating: yoruba-wikipedia-leipzig.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWECRfnWp02P"
      },
      "source": [
        "# Open file\n",
        "#file = io.open('english-shakespeare.txt', encoding='utf8')\n",
        "file = io.open('german-kafka.txt', encoding='utf8')\n",
        "text = file.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YA-utVqO2goN"
      },
      "source": [
        "# Preprocess the tokenized text for language modelling\n",
        "https://www.nltk.org/api/nltk.lm.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_to_english(text):\n",
        "    # Initialize the translator\n",
        "    translator = Translator()\n",
        "\n",
        "    # Detect the language of the input text\n",
        "    lang = translator.detect(text).lang\n",
        "\n",
        "    # If the detected language is not English, translate the text\n",
        "    if lang != 'en':\n",
        "        translation = translator.translate(text, src=lang, dest='en')\n",
        "        translated_text = translation.text\n",
        "    else:\n",
        "        # If the detected language is already English, return the text as is\n",
        "        translated_text = text\n",
        "\n",
        "    return translated_text"
      ],
      "metadata": {
        "id": "-cD3CMUyJpKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate a 100 word texts for unigram, bigram, trigram, quadgram models"
      ],
      "metadata": {
        "id": "gb8xT91jF1Ah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the tokenized text for language modelling\n",
        "n = 1\n",
        "paddedLine = [list(pad_both_ends(word_tokenize(text.lower()), n))]\n",
        "train, vocab = padded_everygram_pipeline(n, paddedLine)\n",
        "\n",
        "# Train a n-gram maximum likelihood estimation model.\n",
        "unimodel = MLE(n)\n",
        "unimodel.fit(train, vocab)\n",
        "\n",
        "#Generate Sequence\n",
        "unigram_gen = unimodel.generate(100, random_seed=42)\n",
        "unigram_seq = ' '.join(unigram_gen)\n",
        "print(\"=== Generated text: unigram ===\")\n",
        "print(unigram_seq)\n",
        "\n",
        "#Generate sequence translation to english\n",
        "english_uni = translate_to_english(unigram_seq)\n",
        "print(\"=== English translation: unigram ===\")\n",
        "print(english_uni)"
      ],
      "metadata": {
        "id": "rIBUKjm6Hfkb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9303b7c4-924d-4be5-e5b7-cf9fe94bdde4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Generated text: unigram ===\n",
            "mir , den bitte schon nicht waren , forschem , beugte hitze , aussichtspunkt mode in bilder kindern theater , suchen oder du als zu draußen , . uns lachte tag schlaf ihre « er in und lärm verstand k. persönlicheres , brechen der , da . denn mich einer einvernehmungen beine den wächter mit lebhaft angelegenheit schien an er » mir irgendeine noch und sie brunelda , die den bekräftigte zeit vor die müde ersoffen wie gesellschaft dem das ist dem katze weg es bewegungen – hätte , , . mannes sondern fort , er übliche ihm « verschiedener\n",
            "=== English translation: unigram ===\n",
            "I, the request, were not already, research, bowed heat, viewpoint fashion in pictures children theater, search or you as too outside ,.We laughed at day sleep and noise k.more personal, break the, there.Because I am happy to have the guard with lively matter.Mannes but away, he uses “various\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqfeSN9X2eOl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd34eafb-7029-4a34-d641-467246c7adc7"
      },
      "source": [
        "# Preprocess the tokenized text for language modelling\n",
        "n = 2\n",
        "paddedLine = [list(pad_both_ends(word_tokenize(text.lower()), n))]\n",
        "train, vocab = padded_everygram_pipeline(n, paddedLine)\n",
        "\n",
        "# Train a n-gram maximum likelihood estimation model.\n",
        "bimodel = MLE(n)\n",
        "bimodel.fit(train, vocab)\n",
        "\n",
        "#Generate Sequence\n",
        "bigram_gen = bimodel.generate(100, random_seed=42)\n",
        "bigram_seq = ' '.join(bigram_gen)\n",
        "print(\"=== Generated text: bigram ===\")\n",
        "print(bigram_seq)\n",
        "\n",
        "#Generate sequence translation to english\n",
        "english_bi = translate_to_english(bigram_seq)\n",
        "print(\"=== English translation: bigram ===\")\n",
        "print(english_bi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Generated text: bigram ===\n",
            "mir , desto deutlicher zu schmal wie das hielt , den kirchendiener , daß ich kein elegantes kleid und aas mißachten sie. « , zeigte auf alle andern verantwortung für seine sache ist zogen sich jemand sonst nicht viel niedriger beamter , denn die augenbrauen drang , die nur ein ihm dadurch darf sich nehmen ihnen auch nur dann förmlich zusammengewachsene felsenharte schutthaufen zu und so erbärmlich. « , der erde ununterscheidbar von der onkel eine von einzelheiten den er ließ aber kannst nicht etwa den übertriebenen mitleid , als daß ich sie hob , er überdies immer zugänglich sein\n",
            "=== English translation: bigram ===\n",
            "me, the more clearly too narrow as that held, the church servant that I am not an elegant dress and AAS displaced them.«, Pointed to all other responsibilities for his cause, someone else did not pull out much lower, because the eyebrows that only came up with one of them can only take them up to them and so grown -hard rubble heaps and so pathetic.«, The earth indispensable from the uncle one of details he left but cannot be the exaggerated pity for lifting it, he also always accessible\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the tokenized text for language modelling\n",
        "n = 3\n",
        "paddedLine = [list(pad_both_ends(word_tokenize(text.lower()), n))]\n",
        "train, vocab = padded_everygram_pipeline(n, paddedLine)\n",
        "\n",
        "# Train a n-gram maximum likelihood estimation model.\n",
        "trimodel = MLE(n)\n",
        "trimodel.fit(train, vocab)\n",
        "\n",
        "#Generate Sequence\n",
        "trigram_gen = trimodel.generate(100, random_seed=42)\n",
        "trigram_seq = ' '.join(trigram_gen)\n",
        "print(\"=== Generated text: trigram ===\")\n",
        "print(trigram_seq)\n",
        "\n",
        "#Generate sequence translation to english\n",
        "english_tri = translate_to_english(trigram_seq)\n",
        "print(\"=== English translation: trigram ===\")\n",
        "print(english_tri)"
      ],
      "metadata": {
        "id": "F34ee1eyH4IA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08275b5c-0795-40db-a7f1-3d70d53250f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Generated text: trigram ===\n",
            "mir , deinetwegen will ich nicht widerstehen . nicht alle bestraft , es hervorzubringen , selbst dir gegenüber verhalten ? « » gewiß , wohin du fährst , hinter dieser vornehmen gesellschaft unbemerkt hinauszukommen . schon steht sie da nicht wie , denkt blumfeld , der aus der parfümflasche . » du fährst mit « , sagte k. , rümpfte die lippen lange liegen . ich fühlte an meiner lage nichts . wenn es notwendig ist . frische fülle . quellende wasser . » ich gehe überhaupt schon ins gekröse greifen und würde hier – natürlich nicht mehr zurückhalten und\n",
            "=== English translation: trigram ===\n",
            "I don't want to resist me because of your sake.Not all punished to produce it, behave towards you?\"\" Certainly where you go to get out behind this noble company unnoticed.Already she is not like, Blumfeld thinks that from the perfume bottle.\"You are driving,\" said K., wrinkled the lips for a long time.I didn't feel anything about my location.if it's necessary .Fresh abundance.swelling water.»I go crane at all and would - of course not hold back and here - of course\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the tokenized text for language modelling\n",
        "n = 4\n",
        "paddedLine = [list(pad_both_ends(word_tokenize(text.lower()), n))]\n",
        "train, vocab = padded_everygram_pipeline(n, paddedLine)\n",
        "\n",
        "# Train a n-gram maximum likelihood estimation model.\n",
        "quadmodel = MLE(n)\n",
        "quadmodel.fit(train, vocab)\n",
        "\n",
        "#Generate Sequence\n",
        "quadgram_gen = quadmodel.generate(100, random_seed=42)\n",
        "quadgram_seq = ' '.join(quadgram_gen)\n",
        "print(\"=== Generated text: quadgram ===\")\n",
        "print(quadgram_seq)\n",
        "\n",
        "#Generate sequence translation to english\n",
        "english_quad = translate_to_english(quadgram_seq)\n",
        "print(\"=== English translation: quadgram ===\")\n",
        "print(english_quad)"
      ],
      "metadata": {
        "id": "qmJVi6J9IBzg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8baaa4bc-5fc7-4a7d-b901-4b1133bc9ead"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Generated text: quadgram ===\n",
            "mir , deinetwegen will ich fort ; weil ich mich an einer eisernen klammer festhielt , die oben in der luft herumschwingen können . jetzt aber , da sie nicht immerfort hinken kann , erfindet sie etwas anderes , etwas eigentlich wunderbares , die speise fiel nicht , sondern gingen zu dem nebenlift und blieben dort , sich umschlungen haltend . herr samsa nickte ihm bloß mehrmals kurz mit großen augen zu . daraufhin ging der herr tatsächlich sofort mit langen schritten ins vorzimmer ; seine beiden freunde hatten schon ein weilchen vorher , zu ertasten gesucht . barnabas gab\n",
            "=== English translation: quadgram ===\n",
            "I want to go away because of you;Because I held on to an iron bracket that can swing in the air at the top.But now, since she cannot always lag, she invents something else, something really wonderful, the food did not fell, but went to the side lift and stayed there.Mr. Samsa just briefly nodded him with big eyes several times.Then the gentleman actually went into the anteroom with long steps;His two friends had been looking for a little before.Barnabas gave\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ngram Evaluation\n",
        "The performance of n-gram models can be evaluated based on the coherence, naturalness, and understandability of the generated text. Typically, higher n-gram models (e.g., n = 3 or n = 4) capture more context and dependencies, leading to more coherent and understandable text compared to lower n-gram models (e.g., n = 1 or n = 2).\n",
        "\n",
        "The higher n-gram models performed better because it can better capture long-range dependencies and context in the text, resulting in more logical and coherent output. Lower n-gram models produced \"bad\" output by generating text that lacks context, coherence, and naturalness, leading to fragmented or nonsensical sentences."
      ],
      "metadata": {
        "id": "pJwfYxoCOzDG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Probabilites and Counts"
      ],
      "metadata": {
        "id": "oLgwaKa_1ov8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Code to find the most common trigrams"
      ],
      "metadata": {
        "id": "88jNAmeSbOoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# Download NLTK resources (you only need to run this once)\n",
        "nltk.download('punkt')\n",
        "\n",
        "def find_most_common_trigrams(file_path, num_trigrams):\n",
        "    # Read the text file\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Filter out special characters and numbers\n",
        "    words = [word for word in words if re.match(r'^[a-zA-Z]+$', word)]\n",
        "\n",
        "    # Generate trigrams from the words\n",
        "    trigrams = [(words[i], words[i+1], words[i+2]) for i in range(len(words)-2)]\n",
        "\n",
        "    # Count the occurrences of each trigram\n",
        "    trigram_counts = Counter(trigrams)\n",
        "\n",
        "    # Filter out trigrams with special characters\n",
        "    filtered_trigrams = [(trigram, count) for trigram, count in trigram_counts.items()\n",
        "                         if all(word.isalpha() for word in trigram)]\n",
        "\n",
        "    # Find the most common trigrams\n",
        "    most_common_trigrams = sorted(filtered_trigrams, key=lambda x: x[1], reverse=True)[:num_trigrams]\n",
        "\n",
        "    return most_common_trigrams\n",
        "\n",
        "# Example usage:\n",
        "from google.colab import files\n",
        "\n",
        "# Upload your text file to Google Colab\n",
        "#uploaded = files.upload()\n",
        "\n",
        "# Get the file name\n",
        "#file_name = next(iter(uploaded))\n",
        "file_name = file.name\n",
        "print(file_name)\n",
        "\n",
        "# Specify the number of most common trigrams you want to find\n",
        "num_trigrams = 10  # Change this number as needed\n",
        "\n",
        "# Find the most common trigrams without special characters\n",
        "most_common_trigrams = find_most_common_trigrams(file_name, num_trigrams)\n",
        "print(f\"Most common {num_trigrams} trigrams without special characters:\")\n",
        "for trigram, count in most_common_trigrams:\n",
        "    print(trigram, \"-\", count)\n"
      ],
      "metadata": {
        "id": "kexFXqNI9hWc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f94085ef-3428-498b-d10e-ba7759dc0154"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "german-kafka.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most common 10 trigrams without special characters:\n",
            "('hie', 'und', 'da') - 97\n",
            "('in', 'der', 'Nacht') - 75\n",
            "('mit', 'der', 'Hand') - 59\n",
            "('in', 'der', 'Hand') - 51\n",
            "('auf', 'und', 'ab') - 47\n",
            "('hin', 'und', 'her') - 46\n",
            "('Es', 'war', 'ein') - 46\n",
            "('sagte', 'die', 'Wirtin') - 46\n",
            "('sagte', 'der', 'Mann') - 44\n",
            "('sagte', 'der', 'Maler') - 44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_ngrams_from_text(text, n):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "\n",
        "    # Generate n-grams\n",
        "    ngrams = [tuple(words[i:i+n]) for i in range(len(words)-n+1)]\n",
        "\n",
        "    # Count the occurrences of each n-gram\n",
        "    ngram_counts = Counter(ngrams)\n",
        "\n",
        "    return ngram_counts"
      ],
      "metadata": {
        "id": "qr-AL58pB0wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the n-grams you want to count\n",
        "unigram_counts = count_ngrams_from_text(text, 1)\n",
        "bigram_counts = count_ngrams_from_text(text, 2)\n",
        "trigram_counts = count_ngrams_from_text(text, 3)\n",
        "\n",
        "# Count occurrences of specific n-grams\n",
        "#phrase = \"I pray you\"\n",
        "phrase = \"in der Nacht\"\n",
        "unigram = phrase.lower().split()[0]  # Convert to lowercase for consistent matching\n",
        "bigram = \" \".join(phrase.lower().split()[:2])  # Convert to lowercase and form bigram\n",
        "trigram = \" \".join(phrase.lower().split())  # Convert to lowercase and form trigram\n",
        "\n",
        "unigram_count = unigram_counts.get((unigram,), 0)\n",
        "bigram_count = bigram_counts.get(tuple(bigram.split()), 0)\n",
        "trigram_count = trigram_counts.get(tuple(trigram.split()), 0)\n",
        "\n",
        "print(\"=== Counts ===\")\n",
        "print(\"Unigram count (\", unigram, \"): \", unigram_count)\n",
        "print(\"Bigram count (\", bigram, \"): \", bigram_count)\n",
        "print(\"Trigram count (\", trigram, \"): \", trigram_count)"
      ],
      "metadata": {
        "id": "OwUfvEaHfxKV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "348932c6-b0d6-4e23-a3c9-91bd2454b3af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Counts ===\n",
            "Unigram count ( in ):  5816\n",
            "Bigram count ( in der ):  1100\n",
            "Trigram count ( in der nacht ):  76\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get words to check\n",
        "word_to_check = trigram.split()\n",
        "\n",
        "# Get the probability of each word in the trigram model\n",
        "word_prob_unigram = trimodel.score(word_to_check[0])\n",
        "word_prob_bigram = trimodel.score(word_to_check[1], context=[word_to_check[0]])\n",
        "word_prob_trigram = trimodel.score(word_to_check[2], context=[word_to_check[0], word_to_check[1]])\n",
        "\n",
        "print(\"=== Probabilities ===\")\n",
        "print(\"Unigram Probability (\", word_to_check[0], \"):\", word_prob_unigram)\n",
        "print(\"Bigram Probability (\", word_to_check[0], word_to_check[1], \"):\", word_prob_bigram)\n",
        "print(\"Trigram Probability (\", trigram, \"):\", word_prob_trigram)"
      ],
      "metadata": {
        "id": "Zr_xvu6eG1u5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b5d8948-5952-4798-9ed1-27cdd36c4dd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Probabilities ===\n",
            "Unigram Probability ( in ): 0.010404864314340383\n",
            "Bigram Probability ( in der ): 0.1892310338895579\n",
            "Trigram Probability ( in der nacht ): 0.06909090909090909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OKc6wePPL4eC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}